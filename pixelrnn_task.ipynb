{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple PixelRNN on digits dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task we will try to train [PixelRNN](https://arxiv.org/pdf/1601.06759.pdf) on sklearn's digits dataset (sklearn.datasets.load_digits). \n",
    "\n",
    "Your task is to implement `Linear`, `ReLU`, `LogSoftMax`, `CrossEntropyLoss` and `SimpleReluRNN` modules (forward pass and gradient computations using numpy) and sampling code to sample from the network output distribution and for the reconstruction example at the end of this notebook. Read all the comments and look for instructions in TODOs. Please delegate as much logic to numpy as you can in forward and backward pass computations to speed up training on CPU. It is possible to implement all modules except SimpleReluRNN without for-loops and SimpleReluRNN with a for-loop over time.\n",
    "\n",
    "There are tests included to test your implementation against PyTorch.\n",
    "\n",
    "After filling out this notebook, please send it to s2t \\[at\\] tinkoff.ru **with your name and \\[Test task\\] tag in the subject line**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "    \"\"\"All neural network modules inherit from this class.\"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize parameters and gradients here.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def forward(self, module_input):\n",
    "        \"\"\"Forward pass: calculate module output based on its input\n",
    "        \n",
    "        This method may share any intermediate state with backward pass via `self`\n",
    "        Args:\n",
    "            module_input: numpy array, input to this module\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"forward not implemented\")\n",
    "\n",
    "    def backward(self, module_input, grad_output):\n",
    "        \"\"\"Backward pass: calculate gradients w.r.t. module parameters and its input.\n",
    "        \n",
    "        No need to override this method, you can override backward_grad_input and (if module has params)\n",
    "        backward_grad_params separatly.\n",
    "        Args:\n",
    "            module_input: numpy array, input to this module, the same array that was passed to forward\n",
    "            grad_output: numpy array of shape self.forward(module_input).shape, gradients w.r.t. module output\n",
    "        \"\"\"\n",
    "        grad_input = self.backward_grad_input(module_input, grad_output)\n",
    "        self.backward_grad_params(module_input, grad_output)\n",
    "        return grad_input\n",
    "    \n",
    "    def backward_grad_input(self, module_input, grad_output):\n",
    "        \"\"\"Calculate gradients w.r.t. input and return them. The returned array must be of the same shape as\n",
    "        module_input. This method may share any intermediate state with backward_grad_params via `self`.\"\"\"\n",
    "        raise NotImplementedError(\"backward_grad_input not implemented\")\n",
    "        \n",
    "    def backward_grad_params(self, module_input, grad_output):\n",
    "        \"\"\"Calculate gradients w.r.t. module parameters and save them for later retrieval via self.grad_params.\"\"\"\n",
    "        pass\n",
    "        \n",
    "    def zero_grad_params(self): \n",
    "        \"\"\"Zero out accumulated gradients.\"\"\"\n",
    "        pass\n",
    "        \n",
    "    @property\n",
    "    def params(self):\n",
    "        \"\"\"Return list of module paramteres. Each parameter is a numpy array of arbitrary shape.\"\"\"\n",
    "        return []\n",
    "        \n",
    "    @property\n",
    "    def grad_params(self):\n",
    "        \"\"\"Return list of accumulated gradients w.r.t. parameters.\n",
    "        \n",
    "        Returned list should be of the same length as self.params. Each element in list must be of the same shape\n",
    "        as corresponding element in self.params\"\"\"\n",
    "        return []\n",
    "\n",
    "\n",
    "class Sequential(Module):\n",
    "    \"\"\"Example implementation of Sequential module.\n",
    "    \n",
    "    Sequential module acts like [torch.nn.Sequential](https://pytorch.org/docs/stable/nn.html#sequential).\n",
    "    \"\"\"\n",
    "    def __init__ (self, *modules):\n",
    "        self._modules = modules\n",
    "        self._module_outputs = []\n",
    "\n",
    "    def forward(self, module_input):\n",
    "        self._module_outputs.clear()\n",
    "        for module in self._modules:\n",
    "            module_input = module.forward(module_input)\n",
    "            self._module_outputs.append(module_input)\n",
    "        return module_input\n",
    "\n",
    "    def backward(self, module_input, grad_output):\n",
    "        modules_reversed = list(reversed(self._modules))\n",
    "        for module, prev_module_output in zip(modules_reversed, self._module_outputs[:-1][::-1]):\n",
    "            grad_output = module.backward(prev_module_output, grad_output)\n",
    "        grad_input = self._modules[0].backward(module_input, grad_output)\n",
    "        return grad_input\n",
    "      \n",
    "    def zero_grad_params(self): \n",
    "        for module in self._modules:\n",
    "            module.zero_grad_params()\n",
    "    \n",
    "    @property\n",
    "    def params(self):\n",
    "        return [module.params for module in self._modules]\n",
    "    \n",
    "    @property\n",
    "    def grad_params(self):\n",
    "        return [module.grad_params for module in self._modules]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    \"\"\"Linear module is the simple module that does the following computation:\n",
    "    \n",
    "    y = xW^T + b,\n",
    "    \n",
    "    where \n",
    "        * x is module input of shape (num_samples, n_in),\n",
    "        * y is module output of shape (num_samples, n_out),\n",
    "        * W is parameter matrix of shape (n_out, n_in) and\n",
    "        * b is bias vector of shape (n_out,)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_in, n_out):\n",
    "        # This is a nice initialization\n",
    "        stdv = 1.0 / np.sqrt(n_in)\n",
    "        self.W = np.random.uniform(-stdv, stdv, size=(n_out, n_in))\n",
    "        self.b = np.random.uniform(-stdv, stdv, size=n_out)\n",
    "        self.grad_W = np.zeros_like(self.W)\n",
    "        self.grad_b = np.zeros_like(self.b)\n",
    "        \n",
    "    def forward(self, module_input):\n",
    "        # TODO: calculate module output using the formula above\n",
    "        # TODO: add your implementation here\n",
    "        pass\n",
    "    \n",
    "    def backward_grad_input(self, module_input, grad_output):\n",
    "        # TODO: calculate gradients w.r.t. x (dL/dx, where L is some loss function) based\n",
    "        # on x and gradients w.r.t. y (dL/dy). Derive neccessary formulas. \n",
    "        # TODO: add your implementation here\n",
    "        pass\n",
    "    \n",
    "    def backward_grad_params(self, module_input, grad_output):\n",
    "        # TODO: Calculate gradients w.r.t. W and b (dL/dW, dL/db) based on x and dL/dy\n",
    "        # TODO: add your implementation here\n",
    "        pass\n",
    "    \n",
    "    def zero_grad_params(self):\n",
    "        self.grad_W.fill(0)\n",
    "        self.grad_b.fill(0)\n",
    "    \n",
    "    @property\n",
    "    def params(self):\n",
    "        return [self.W, self.b]\n",
    "    \n",
    "    @property\n",
    "    def grad_params(self):\n",
    "        return [self.grad_W, self.grad_b]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    \"\"\"ReLU is a module that implements the following function:\n",
    "    \n",
    "    y = max(x, 0)\n",
    "    \"\"\"\n",
    "    def forward(self, module_input):\n",
    "        # TODO: calculate module output based on the formula above\n",
    "        # TODO: add your implementation here\n",
    "        pass\n",
    "    \n",
    "    def backward_grad_input(self, module_input, grad_output):\n",
    "        # TODO: calculate gradients w.r.t. x based on gradients w.r.t. y\n",
    "        # TODO: add your implementation here\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogSoftMax(Module):\n",
    "    \"\"\"LogSoftMax is a module that implements natural logarithm of softmax function\n",
    "    (https://en.wikipedia.org/wiki/Softmax_function, https://pytorch.org/docs/stable/nn.html#torch.nn.LogSoftmax)\n",
    "    \n",
    "    y_ij = log_softmax(x)_ij = log(exp(x_ij) / \\sum_k(exp(x_ik)))\n",
    "    \n",
    "    where x is the module input of shape (num_samples, num_classes) and y is the module output of the same shape\n",
    "    \n",
    "    LogSoftMax is invariant to scalar shifts:\n",
    "    \n",
    "    log_softmax(x + \\alpha)_ij = log(exp(x_ij + \\alpha) / \\sum_k(exp(x_ik + \\alpha))) = \n",
    "    log(exp(x_ij) / \\sum_k(exp(x_ik))) = log_softmax(x)_ij\n",
    "    \n",
    "    For numerical stability, it is important to normalize the input before calculations\n",
    "    (set \\alpha_ij = max_j(x_ij) in the scalar shift invariance formula). \n",
    "    \"\"\"\n",
    "    def forward(self, module_input):\n",
    "        # TODO: Start with normalization for numerical stability, then implement logsoftmax based on formula above.\n",
    "        # You may want to save module output for later reuse in backward pass computations. Scalar shift is invisible\n",
    "        # for gradient calculations, so you don't need to keep track of normalization constant for later use in\n",
    "        # backward pass.\n",
    "        # TODO: add your implementation here\n",
    "        pass\n",
    "    \n",
    "    def backward_grad_input(self, module_input, grad_output):\n",
    "        # TODO: implement gradients w.r.t. x based on gradients w.r.t. y\n",
    "        # TODO: add your implementation here\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss(Module):\n",
    "    \"\"\"CrossEntropyLoss is a module that implements cross entropy function between some distribution and\n",
    "    a target distribution (typically one-hot encoded). See https://en.wikipedia.org/wiki/Cross_entropy\n",
    "    \n",
    "    y = - \\sum_ij(target_ij * logprob_ij) / logprob.shape[0]\n",
    "    \n",
    "    where:\n",
    "        * target is a numpy array of shape (num_samples, num_classes), target distribution, typically one-hot encoded\n",
    "        * logprob is a numpy array of shape (num_samples, num_classes), logarithm of some probability distribution, \n",
    "            typically output of LogSoftMax layer\n",
    "        * y is a scalar, the final cross entropy between logprob and target\n",
    "    \"\"\"\n",
    "    def forward(self, logprob, target):\n",
    "        # TODO: calculate cross entropy using the formula above.\n",
    "        # TODO: add your implementation here\n",
    "        pass\n",
    "\n",
    "    def backward_grad_input(self, logprob, target):\n",
    "        # TODO: calculate gradients w.r.t. logprob: (dy/dlogprob)\n",
    "        # TODO: add your implementation here\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleReluRNN(Module):\n",
    "    \"\"\"SimpleReluRNN is a module that implements vanilla rnn with relu activation\n",
    "    (https://pytorch.org/docs/stable/nn.html#torch.nn.RNN)\n",
    "    \n",
    "    y_t = h_t = max(Wx_t + Uh_{t-1}, 0)\n",
    "    \n",
    "    where:\n",
    "        X = {x_1, x_2, ...} is a module input of shape (num_samples, input_size)\n",
    "        H = {h_1, h_2, ...} is a module hidden states of shape (num_samples, hidden_size)\n",
    "        Y = {y_1, y_2, ...} is a module output of shape (num_samples, hidden_size)\n",
    "        \n",
    "    h_0 is typically initialized with zeros (np.zeros(hidden_size))\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        stdv_W = 1 / np.sqrt(input_size)\n",
    "        self.W = np.random.uniform(-stdv_W, stdv_W, size=(hidden_size, input_size))\n",
    "        stdv_U = 1 / np.sqrt(hidden_size)\n",
    "        self.U = np.random.uniform(-stdv_U, stdv_U, size=(hidden_size, hidden_size))\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.grad_W = np.zeros_like(self.W)\n",
    "        self.grad_U = np.zeros_like(self.U)\n",
    "        \n",
    "    def step(self, input_slice, hidden):\n",
    "        # TODO: implement rnn step: h_t = relu(Wx_t + Uh_{t-1})\n",
    "        # TODO: add your code here\n",
    "        pass\n",
    "\n",
    "    def forward(self, module_input):\n",
    "        # TODO: implement forward pass: calculate {h_1, h_2, ... h_t} based on module_input using self.step\n",
    "        # and return np.stack([h_1, h_2, ..., h_t])\n",
    "        # TODO: you may want to save module output for later use in backward calculations\n",
    "        # TODO: add your code here\n",
    "        pass\n",
    "\n",
    "    def backward_grad_input(self, module_input, grad_output):\n",
    "        # TODO: calculate gradients w.r.t. module input (dL/dx) based on gradients w.r.t. module output (dL/dY)\n",
    "        # TODO: note that each element in grad_output is not the full derivative of L w.r.t. h_t, because\n",
    "        # h_t is \n",
    "        #   1) returned from the module (y_t = h_t)\n",
    "        #   2) used in computations at later timestamps\n",
    "        # you need to aggregate both of this cases to obtain the full derivative (dL/dH) in a similar manner \n",
    "        # as in forward pass (for-loop over time in reverse direction)\n",
    "        # You may want to save dL/dH in self for later use in backward_grad_params\n",
    "        # TODO: add your code here\n",
    "        pass\n",
    "\n",
    "    def backward_grad_params(self, module_input, grad_output):\n",
    "        # TODO: calculate gradients w.r.t. module params (dL/dW, dL/dU). Because W and U is used at all timestamps,\n",
    "        # gradients need to be aggregated from each timestamp\n",
    "        # TODO: add your code here\n",
    "        pass\n",
    "\n",
    "    def zero_grad_params(self): \n",
    "        self.grad_W.fill(0)\n",
    "        self.grad_U.fill(0)\n",
    "\n",
    "    @property\n",
    "    def params(self):\n",
    "        return [self.W, self.U]\n",
    "\n",
    "    @property\n",
    "    def grad_params(self):\n",
    "        return [self.grad_W, self.grad_U]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"Testing your implementation\")\n",
    "\n",
    "def test_linear():\n",
    "    for i in range(10):\n",
    "        layer = Linear(16, 32)\n",
    "        torch_layer = torch.nn.Linear(16, 32)\n",
    "        torch_layer.weight.data = torch.tensor(layer.W)\n",
    "        torch_layer.bias.data = torch.tensor(layer.b)\n",
    "        \n",
    "        inp = np.random.normal(size=(10, 16))\n",
    "        inp_torch = torch.tensor(inp, requires_grad=True)\n",
    "        \n",
    "        result = layer.forward(inp)\n",
    "        result_torch = torch_layer.forward(inp_torch)\n",
    "        assert(np.allclose(result, result_torch.data.numpy()))\n",
    "        \n",
    "        grad_out = np.random.normal(size=(10, 32))\n",
    "        grad_in = layer.backward(inp, grad_out)\n",
    "        \n",
    "        (result_torch * torch.tensor(grad_out)).sum().backward()\n",
    "        grad_in_torch = inp_torch.grad\n",
    "        assert(np.allclose(grad_in, grad_in_torch.data.numpy()))\n",
    "        assert(np.allclose(layer.grad_W, torch_layer.weight.grad.data.numpy()))\n",
    "        assert(np.allclose(layer.grad_b, torch_layer.bias.grad.data.numpy()))\n",
    "        \n",
    "def test_relu():\n",
    "    for i in range(10):\n",
    "        layer = ReLU()\n",
    "        torch_layer = torch.nn.ReLU()\n",
    "        \n",
    "        inp = np.random.normal(size=(10, 16))\n",
    "        inp_torch = torch.tensor(inp, requires_grad=True)\n",
    "        \n",
    "        result = layer.forward(inp)\n",
    "        result_torch = torch_layer.forward(inp_torch)\n",
    "        assert(np.allclose(result, result_torch.data.numpy()))\n",
    "\n",
    "        grad_out = np.random.normal(size=(10, 16))\n",
    "        grad_in = layer.backward(inp, grad_out)\n",
    "        \n",
    "        (result_torch * torch.tensor(grad_out)).sum().backward()\n",
    "        grad_in_torch = inp_torch.grad\n",
    "        assert(np.allclose(grad_in, grad_in_torch.data.numpy()))\n",
    "        \n",
    "        \n",
    "def test_log_softmax():\n",
    "    for i in range(10):\n",
    "        layer = LogSoftMax()\n",
    "        torch_layer = torch.nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        inp = np.random.normal(size=(10, 16))\n",
    "        inp_torch = torch.tensor(inp, requires_grad=True)\n",
    "        \n",
    "        result = layer.forward(inp)\n",
    "        result_torch = torch_layer.forward(inp_torch)\n",
    "        assert(np.allclose(result, result_torch.data.numpy()))\n",
    "        \n",
    "        grad_out = np.random.normal(size=(10, 16))\n",
    "        grad_in = layer.backward(inp, grad_out)\n",
    "        \n",
    "        (result_torch * torch.tensor(grad_out)).sum().backward()\n",
    "        grad_in_torch = inp_torch.grad\n",
    "        assert(np.allclose(grad_in, grad_in_torch.data.numpy()))\n",
    "        \n",
    "def test_cross_entropy_loss():\n",
    "    for i in range(10):\n",
    "        layer = CrossEntropyLoss()\n",
    "        torch_layer = torch.nn.NLLLoss()\n",
    "        \n",
    "        inp = np.random.normal(size=(10, 16))\n",
    "        inp_torch = torch.tensor(inp, requires_grad=True)\n",
    "        \n",
    "        target_indices = np.random.choice(16, 10)\n",
    "        target_ohe = np.zeros((10, 16), dtype=np.int64)\n",
    "        target_ohe[np.arange(10), target_indices] = 1\n",
    "        \n",
    "        result = layer.forward(inp, target_ohe)\n",
    "        result_torch = torch_layer.forward(inp_torch, torch.tensor(target_indices))\n",
    "        \n",
    "        assert(np.allclose(result, result_torch.data.numpy()))\n",
    "        \n",
    "        grad_in = layer.backward(inp, target_ohe)\n",
    "        \n",
    "        result_torch.backward()\n",
    "        grad_in_torch = inp_torch.grad\n",
    "        assert(np.allclose(grad_in, grad_in_torch.data.numpy()))\n",
    "        \n",
    "        \n",
    "def test_rnn_relu():\n",
    "    for i in range(10):\n",
    "        layer = SimpleReluRNN(64, 128)\n",
    "        torch_layer = torch.nn.RNN(64, 128, nonlinearity='relu', bias=False)\n",
    "        torch_layer.weight_ih_l0.data = torch.tensor(layer.W)\n",
    "        torch_layer.weight_hh_l0.data = torch.tensor(layer.U)\n",
    "        \n",
    "        inp = np.random.normal(size=(10, 64))\n",
    "        inp_torch = torch.tensor(inp[:, None, :], requires_grad=True)\n",
    "        \n",
    "        result = layer.forward(inp)\n",
    "        result_torch = torch_layer.forward(inp_torch)[0][:, 0, :]\n",
    "        assert(np.allclose(result, result_torch.data.numpy()))\n",
    "        \n",
    "        grad_out = np.random.normal(size=(10, 128))\n",
    "        grad_in = layer.backward(inp, grad_out)\n",
    "        \n",
    "        (result_torch * torch.tensor(grad_out)).sum().backward()\n",
    "        grad_in_torch = inp_torch.grad[:, 0, :]\n",
    "        assert(np.allclose(grad_in, grad_in_torch.data.numpy()))\n",
    "        assert(np.allclose(layer.grad_W, torch_layer.weight_ih_l0.grad.data.numpy()))\n",
    "        assert(np.allclose(layer.grad_U, torch_layer.weight_hh_l0.grad.data.numpy()))\n",
    "        \n",
    "def test_sequential():\n",
    "    for i in range(10):\n",
    "        linear_1 = Linear(8, 16)\n",
    "        linear_2 = Linear(16, 32)\n",
    "        model = Sequential(\n",
    "            linear_1,\n",
    "            LogSoftMax(),\n",
    "            linear_2,\n",
    "            LogSoftMax()\n",
    "        )\n",
    "        linear_1_torch = torch.nn.Linear(8, 16)\n",
    "        linear_1_torch.weight.data = torch.tensor(linear_1.W)\n",
    "        linear_1_torch.bias.data = torch.tensor(linear_1.b)\n",
    "        linear_2_torch = torch.nn.Linear(16, 32)\n",
    "        linear_2_torch.weight.data = torch.tensor(linear_2.W)\n",
    "        linear_2_torch.bias.data = torch.tensor(linear_2.b)\n",
    "        model_torch = torch.nn.Sequential(\n",
    "            linear_1_torch,\n",
    "            torch.nn.LogSoftmax(dim=-1),\n",
    "            linear_2_torch,\n",
    "            torch.nn.LogSoftmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        inp = np.random.normal(size=(10, 8))\n",
    "        inp_torch = torch.tensor(inp, requires_grad=True)\n",
    "        \n",
    "        result = model.forward(inp)\n",
    "        result_torch = model_torch.forward(inp_torch)\n",
    "        \n",
    "        assert(np.allclose(result, result_torch.data.numpy()))\n",
    "        \n",
    "        grad_out = np.random.normal(size=(10, 32))\n",
    "        grad_in = model.backward(inp, grad_out)\n",
    "        \n",
    "        (result_torch * torch.tensor(grad_out)).sum().backward()\n",
    "        grad_in_torch = inp_torch.grad\n",
    "        assert(np.allclose(grad_in, grad_in_torch.data.numpy()))\n",
    "        assert(np.allclose(linear_1.grad_W, linear_1_torch.weight.grad.data.numpy()))\n",
    "        assert(np.allclose(linear_1.grad_b, linear_1_torch.bias.grad.data.numpy()))\n",
    "        assert(np.allclose(linear_2.grad_W, linear_2_torch.weight.grad.data.numpy()))\n",
    "        assert(np.allclose(linear_2.grad_b, linear_2_torch.bias.grad.data.numpy()))\n",
    "        \n",
    "        \n",
    "test_linear()\n",
    "test_relu()\n",
    "test_log_softmax()\n",
    "test_cross_entropy_loss()\n",
    "test_rnn_relu()\n",
    "test_sequential()\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PixelRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDOptimizer:\n",
    "    \"\"\"Simple optimizer that does stochastic gradient descent with momentum update rule.\"\"\"\n",
    "    def __init__(self, model, lr, momentum=0.9):\n",
    "        self._lr = lr\n",
    "        self._momentum = momentum\n",
    "        self._model = model\n",
    "        self._grad_running_mean = {}\n",
    "        for layer in self._model.params:\n",
    "            for param in layer:\n",
    "                self._grad_running_mean[id(param)] = np.zeros_like(param)\n",
    "    \n",
    "    def step(self):\n",
    "        for layer, layer_grads in zip(self._model.params, self._model.grad_params):\n",
    "            for param, param_grad in zip(layer, layer_grads):\n",
    "                self._grad_running_mean[id(param)] = ((1 - self._momentum) * param_grad +\n",
    "                                                      self._momentum * self._grad_running_mean[id(param)])\n",
    "                param -= self._lr * self._grad_running_mean[id(param)]\n",
    "        self._model.zero_grad_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(Module):\n",
    "    \"\"\"Here is the most simple PixelRNN implementation.\"\"\"\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self._rnn = SimpleReluRNN(input_size, hidden_size)\n",
    "        self._prediction_net = Sequential(\n",
    "            Linear(hidden_size, hidden_size),\n",
    "            ReLU(),\n",
    "            Linear(hidden_size, input_size),\n",
    "            LogSoftMax()\n",
    "        )\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        self._rnn_out = self._rnn.forward(inp)\n",
    "        self._output = self._prediction_net.forward(self._rnn_out)\n",
    "        return self._output\n",
    "    \n",
    "    def backward(self, inp, grad_out):\n",
    "        grad_rnn_out = self._prediction_net.backward(self._rnn_out, grad_out)\n",
    "        grad_in = self._rnn.backward(inp, grad_rnn_out)\n",
    "        return grad_in\n",
    "    \n",
    "    def zero_grad_params(self):\n",
    "        self._rnn.zero_grad_params()\n",
    "        self._prediction_net.zero_grad_params()\n",
    "        \n",
    "    def step(self, inp, hidden):\n",
    "        hidden = self._rnn.step(inp, hidden)\n",
    "        loglikes = self._prediction_net.forward(hidden[None, :])[0]\n",
    "        return loglikes, hidden\n",
    "    \n",
    "    @property\n",
    "    def params(self):\n",
    "        return [self._rnn.params, *self._prediction_net.params]\n",
    "    \n",
    "    @property\n",
    "    def grad_params(self):\n",
    "        return [self._rnn.grad_params, *self._prediction_net.grad_params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: try changing hidden state size and optimizer parameters (learning rate, momentum) to obtain the\n",
    "# best train set cross entropy loss\n",
    "network = Network(17, 128)\n",
    "opt = SGDOptimizer(network, lr=1e-2, momentum=0.9)\n",
    "loss = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"Loading data\")\n",
    "\n",
    "X = load_digits()['data'].astype(np.int)\n",
    "np.random.shuffle(X)\n",
    "\n",
    "_, axes = plt.subplots(1, 10, figsize=(10, 1))\n",
    "for idx, img in enumerate(X[:10]):\n",
    "    axes[idx].imshow(img.reshape(8, 8), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reference: you should be able to obtain final loss close to 1.2-1.4 if you implemented CrossEntropyLoss\n",
    "# with normalization over samples. If your loss is higher, try tweaking learning_rate and hidden_size parameters\n",
    "print(\"Training for 20 epochs\")\n",
    "\n",
    "for epoch in range(20):\n",
    "    loss_values = []\n",
    "    for idx, sequence in enumerate(X):\n",
    "        X_ohe = np.zeros((len(sequence), 17))\n",
    "        X_ohe[np.arange(len(sequence)), sequence] = 1\n",
    "        inp = X_ohe[:-1]\n",
    "        target = X_ohe[1:]\n",
    "        loglikes = network.forward(inp)\n",
    "        loss_value = loss.forward(loglikes, target)\n",
    "        loss_values.append(loss_value)\n",
    "        grad_loglikes = loss.backward(loglikes, target)\n",
    "        network.backward(inp, grad_loglikes)\n",
    "        opt.step()\n",
    "    print(epoch, np.mean(loss_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(network, num_steps, starting_samples=[0]):\n",
    "    hidden = np.zeros(network._rnn.hidden_size)\n",
    "    samples = [sample for sample in starting_samples]\n",
    "    # TODO: first feed all samples but the last one to the network to obtain correct hidden state,\n",
    "    # then sample your network num_steps times starting with the last element of samples as the first input\n",
    "    # to the network, each time calling np.random.choice(np.arange(17), p=probs) with the correct\n",
    "    # probabilities (np.exp(loglikes))\n",
    "    # TODO: add your code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sampling from network\")\n",
    "plt.figure(figsize=(1, 8))\n",
    "plt.imshow(sample(network, 63).reshape(8, 8), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we try reconstruction example from the paper (https://arxiv.org/pdf/1601.06759.pdf)\n",
    "print(\"Reconstruction example\")\n",
    "_, axes = plt.subplots(1, 5)\n",
    "img = X[0]\n",
    "axes[0].imshow(img.reshape(8, 8), cmap=\"gray\")\n",
    "axes[0].set_title(\"Original\")\n",
    "upper_part = img[:32]\n",
    "axes[1].imshow(np.concatenate((upper_part.reshape(4, 8),\n",
    "                               np.zeros(shape=(4, 8)))), cmap=\"gray\")\n",
    "axes[1].set_title(\"Occluded\")\n",
    "result = sample(network, 32, starting_samples=upper_part)\n",
    "axes[2].imshow(result.reshape(8, 8), cmap=\"gray\")\n",
    "axes[2].set_title(\"Sample #1\")\n",
    "\n",
    "result = sample(network, 32, starting_samples=upper_part)\n",
    "axes[3].imshow(result.reshape(8, 8), cmap=\"gray\")\n",
    "axes[3].set_title(\"Sample #2\")\n",
    "\n",
    "result = sample(network, 32, starting_samples=upper_part)\n",
    "axes[4].imshow(result.reshape(8, 8), cmap=\"gray\")\n",
    "axes[4].set_title(\"Sample #3\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
